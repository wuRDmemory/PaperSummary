Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Potje2024,
abstract = {We introduce a lightweight and accurate architecture for resource-efficient visual correspondence. Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features. Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices. In particular, accurate image matching requires sufficiently large image resolutions - for this reason, we keep the resolution as large as possible while limiting the number of channels in the network. Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality. Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization. We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations. Code and weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24.},
archivePrefix = {arXiv},
arxivId = {2404.19174},
author = {Potje, Guilherme and Cadar, Felipe and Araujo, Andre and Martins, Renato and Nascimento, Erickson R.},
eprint = {2404.19174},
file = {:C\:/Users/wuch2/Nutstore/1/papers/xfeat.pdf:pdf},
title = {{XFeat: Accelerated Features for Lightweight Image Matching}},
url = {http://arxiv.org/abs/2404.19174},
year = {2024}
}
@article{Sola2018,
abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
archivePrefix = {arXiv},
arxivId = {1812.01537},
author = {Sol{\`{a}}, Joan and Deray, Jeremie and Atchuthan, Dinesh and Sol, Joan and Deray, Jeremie and Atchuthan, Dinesh and Sol{\`{a}}, Joan and Deray, Jeremie and Atchuthan, Dinesh},
eprint = {1812.01537},
pages = {1--17},
title = {{A micro Lie theory for state estimation in robotics}},
url = {http://arxiv.org/abs/1812.01537},
year = {2018}
}
@article{Kohlbrecher2016a,
abstract = {The proceedings contain 41 papers. The topics discussed include: research on autonomous stairs climbing for the shape-shifting robot; modularity for maximum mobility and manipulation: control of a reconfigurable legged robot with series-elastic actuators; a man-packable unmanned surface vehicle for radiation localization and forensics; friction binding study and remedy design for tethered search and rescue robots; shape-constrained whole-body adaptivity; classification of outdoor 3D lidar data based on unsupervised Gaussian mixture models; compliant snake robot locomotion on horizontal pipes; environmental sensing using millimeter wave sensor for extreme conditions; where to place cameras on a snake robot: focus on camera trajectory and motion blur; human-voice enhancement based on online RPCA for a hose-shaped rescue robot with a microphone array; improving the interpretation of thermal images with the aid of emissivity's angular dependency; and people in the weeds: pedestrian detection goes off-road.},
author = {Kohlbrecher, Stefan and Stryk, Oskar Von and Meyer, Johannes and Klingauf, Uwe},
isbn = {9781509019595},
journal = {SSRR 2015 - 2015 IEEE International Symposium on Safety, Security, and Rescue Robotics},
keywords = {inertial,navigation,robust and fast localization,simultaneous localization and mapping},
pages = {0--5},
title = {{SSRR 2015 - 2015 IEEE International Symposium on Safety, Security, and Rescue Robotics}},
year = {2016}
}
@article{Zhou2022,
abstract = {This paper proposes an efficient multi-camera to Bird's-Eye-View (BEV) view transformation method for 3D perception, dubbed MatrixVT. Existing view transformers either suffer from poor transformation efficiency or rely on device-specific operators, hindering the broad application of BEV models. In contrast, our method generates BEV features efficiently with only convolutions and matrix multiplications (MatMul). Specifically, we propose describing the BEV feature as the MatMul of image feature and a sparse Feature Transporting Matrix (FTM). A Prime Extraction module is then introduced to compress the dimension of image features and reduce FTM's sparsity. Moreover, we propose the Ring $\$& Ray Decomposition to replace the FTM with two matrices and reformulate our pipeline to reduce calculation further. Compared to existing methods, MatrixVT enjoys a faster speed and less memory footprint while remaining deploy-friendly. Extensive experiments on the nuScenes benchmark demonstrate that our method is highly efficient but obtains results on par with the SOTA method in object detection and map segmentation tasks},
archivePrefix = {arXiv},
arxivId = {2211.10593},
author = {Zhou, Hongyu and Ge, Zheng and Li, Zeming and Zhang, Xiangyu},
eprint = {2211.10593},
title = {{MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception}},
url = {http://arxiv.org/abs/2211.10593},
year = {2022}
}
@article{Engel2013d,
abstract = {We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to benefit from the simplicity and accuracy of dense tracking-which does not depend on visual features-while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications. {\textcopyright} 2013 IEEE.},
author = {Engel, Jakob and Sturm, Jurgen and Cremers, Daniel},
isbn = {9781479928392},
issn = {00201693},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {SLAM,dense,monocular,stereo,visual odometry},
pages = {1449--1456},
title = {{LSD-SLAM: Large-Scale Direct Monocular SLAM}},
year = {2013}
}
@article{Zhang2023,
abstract = {The vision-based perception for autonomous driving has undergone a transformation from the bird-eye-view (BEV) representations to the 3D semantic occupancy. Compared with the BEV planes, the 3D semantic occupancy further provides structural information along the vertical direction. This paper presents OccFormer, a dual-path transformer network to effectively process the 3D volume for semantic occupancy prediction. OccFormer achieves a long-range, dynamic, and efficient encoding of the camera-generated 3D voxel features. It is obtained by decomposing the heavy 3D processing into the local and global transformer pathways along the horizontal plane. For the occupancy decoder, we adapt the vanilla Mask2Former for 3D semantic occupancy by proposing preserve-pooling and class-guided sampling, which notably mitigate the sparsity and class imbalance. Experimental results demonstrate that OccFormer significantly outperforms existing methods for semantic scene completion on SemanticKITTI dataset and for LiDAR semantic segmentation on nuScenes dataset. Code is available at \url{https://github.com/zhangyp15/OccFormer}.},
archivePrefix = {arXiv},
arxivId = {2304.05316},
author = {Zhang, Yunpeng and Zhu, Zheng and Du, Dalong},
eprint = {2304.05316},
title = {{OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction}},
url = {http://arxiv.org/abs/2304.05316},
year = {2023}
}
@article{2018,
author = {崔华坤},
title = {{ICE-BA论文分析及GBA代码解析by崔华坤_4.0}},
year = {2018}
}
@article{Liu2023,
abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \url{https://github.com/IDEA-Research/GroundingDINO}.},
archivePrefix = {arXiv},
arxivId = {2303.05499},
author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
eprint = {2303.05499},
title = {{Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}},
url = {http://arxiv.org/abs/2303.05499},
year = {2023}
}
@article{Zhang2022,
abstract = {We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\textbf{+6.0}$\textbf{AP} and $\textbf{+2.7}$\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \texttt{val2017} ($\textbf{63.2}$\textbf{AP}) and \texttt{test-dev} (\textbf{$\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \url{https://github.com/IDEACVR/DINO}.},
archivePrefix = {arXiv},
arxivId = {2203.03605},
author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung},
eprint = {2203.03605},
keywords = {detection transformer,end-to-end de-,object detection},
title = {{DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}},
url = {http://arxiv.org/abs/2203.03605},
year = {2022}
}
@article{Wang2023a,
abstract = {Recently, lightweight Vision Transformers (ViTs) demonstrate superior performance and lower latency compared with lightweight Convolutional Neural Networks (CNNs) on resource-constrained mobile devices. This improvement is usually attributed to the multi-head self-attention module, which enables the model to learn global representations. However, the architectural disparities between lightweight ViTs and lightweight CNNs have not been adequately examined. In this study, we revisit the efficient design of lightweight CNNs and emphasize their potential for mobile devices. We incrementally enhance the mobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, by integrating the efficient architectural choices of lightweight ViTs. This ends up with a new family of pure lightweight CNNs, namely RepViT. Extensive experiments show that RepViT outperforms existing state-of-the-art lightweight ViTs and exhibits favorable latency in various vision tasks. On ImageNet, RepViT achieves over 80\% top-1 accuracy with 1ms latency on an iPhone 12, which is the first time for a lightweight model, to the best of our knowledge. Our largest model, RepViT-M2.3, obtains 83.7\% accuracy with only 2.3ms latency. The code and trained models are available at \url{https://github.com/jameslahm/RepViT}.},
archivePrefix = {arXiv},
arxivId = {2307.09283},
author = {Wang, Ao and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang},
eprint = {2307.09283},
title = {{RepViT: Revisiting Mobile CNN From ViT Perspective}},
url = {http://arxiv.org/abs/2307.09283},
year = {2023}
}
@article{Tarrio2019,
abstract = {Vision-based Simultaneous Localization And Mapping (VSLAM) is a mature problem in Robotics. Most VSLAM systems are feature based methods, which are robust and present high accuracy, but yield sparse maps with limited application for further navigation tasks. Most recently, direct methods which operate directly on image intensity have been introduced, capable of reconstructing richer maps at the cost of higher processing power. In this work, an edge-based monocular SLAM system (SE-SLAM) is proposed as a middle point: edges present good localization as point features, while enabling a structural semidense map reconstruction. However, edges are not easy to associate, track and optimize over time, as they lack descriptors and biunivocal correspondence, unlike point features. To tackle these issues, this paper presents a method to match edges between frames in a consistent manner; a feasible strategy to solve the optimization problem, since its size rapidly increases when working with edges; and the use of non-linear optimization techniques. The resulting system achieves comparable precision to state of the art feature-based and dense/semi-dense systems, while inherently building a structural semi-dense reconstruction of the environment, providing relevant structure data for further navigation algorithms. To achieve such accuracy, state of the art non-linear optimization is needed, over a continuous feed of 10000 edgepoints per frame, to optimize the full semi-dense output. Despite its heavy processing requirements, the system achieves near to real-time operation, thanks to a custom built solver and parallelization of its key stages. In order to encourage further development of edge-based SLAM systems, SE-SLAM source code will be released as open source.},
archivePrefix = {arXiv},
arxivId = {1909.03917},
author = {Tarrio, Juan Jose and Smitt, Claus and Pedre, Sol and Jos, Juan and Smitt, Claus and Pedre, Sol},
eprint = {1909.03917},
title = {{SE-SLAM: Semi-Dense Structured Edge-Based Monocular SLAM}},
url = {http://arxiv.org/abs/1909.03917},
year = {2019}
}
@article{Dosovitskiy2020,
abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
archivePrefix = {arXiv},
arxivId = {2010.11929},
author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
eprint = {2010.11929},
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
url = {http://arxiv.org/abs/2010.11929},
year = {2020}
}
@article{Sola2017,
abstract = {This article is an exhaustive revision of concepts and formulas related to quaternions and rotations in 3D space, and their proper use in estimation engines such as the error-state Kalman filter. The paper includes an in-depth study of the rotation group and its Lie structure, with formulations using both quaternions and rotation matrices. It makes special attention in the definition of rotation perturbations, derivatives and integrals. It provides numerous intuitions and geometrical interpretations to help the reader grasp the inner mechanisms of 3D rotation. The whole material is used to devise precise formulations for error-state Kalman filters suited for real applications using integration of signals from an inertial measurement unit (IMU).},
archivePrefix = {arXiv},
arxivId = {1711.02508},
author = {Sol{\`{a}}, Joan and Sol, Joan},
eprint = {1711.02508},
journal = {Laboratoire dAnalyse et dArchitecture des Systemes-Centre national de la recherche scientifique (LAAS-CNRS), Toulouse, France, Tech. Re},
pages = {6},
title = {{Quaternion kinematics for the error-state Kalman filter}},
url = {http://arxiv.org/abs/1711.02508},
year = {2017}
}
@article{Liu2022,
abstract = {In this paper, we propose PETRv2, a unified framework for 3D perception from multi-view images. Based on PETR, PETRv2 explores the effectiveness of temporal modeling, which utilizes the temporal information of previous frames to boost 3D object detection. More specifically, we extend the 3D position embedding (3D PE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on object position of different frames. A feature-guided position encoder is further introduced to improve the data adaptability of 3D PE. To support for multi-task learning (e.g., BEV segmentation and 3D lane detection), PETRv2 provides a simple yet effective solution by introducing task-specific queries, which are initialized under different spaces. PETRv2 achieves state-of-the-art performance on 3D object detection, BEV segmentation and 3D lane detection. Detailed robustness analysis is also conducted on PETR framework. We hope PETRv2 can serve as a strong baseline for 3D perception. Code is available at $\$url{https://github.com/megvii-research/PETR}.},
archivePrefix = {arXiv},
arxivId = {2206.01256},
author = {Liu, Yingfei and Yan, Junjie and Jia, Fan and Li, Shuailin and Gao, Aqi and Wang, Tiancai and Zhang, Xiangyu and Sun, Jian},
eprint = {2206.01256},
title = {{PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images}},
url = {http://arxiv.org/abs/2206.01256},
year = {2022}
}
@article{Barron2022,
author = {Barron, Jonathan T and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P and Hedman, Peter},
pages = {5470--5479},
title = {{Mip-NeRF 360 : Unbounded Anti-Aliased Neural Radiance Fields}},
year = {2022}
}
@article{Yang2021,
abstract = {This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1% J &F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%), while keeping more than 3× faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.},
archivePrefix = {arXiv},
arxivId = {2106.02638},
author = {Yang, Zongxin and Wei, Yunchao and Yang, Yi},
eprint = {2106.02638},
isbn = {9781713845393},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {2491--2502},
title = {{Associating Objects with Transformers for Video Object Segmentation}},
volume = {4},
year = {2021}
}
@article{Geneva2022,
author = {Geneva, Patrick and Huang, Guoquan},
title = {{OpenVINS State Initialization: Details and Derivations}},
year = {2022}
}
@article{Notes2018a,
author = {Notes, Com S},
pages = {1--11},
title = {{Pl ¨ ucker Coordinates for Lines in the Space ∗ Pl ¨ ucker Coordinates_2018_Notes.pdf}},
year = {2018}
}
@book{Lang1996,
author = {Lang, Serge},
isbn = {0387948414},
pages = {668},
title = {{Linear Algebra Done Right}},
url = {http://www.amazon.com/Undergraduate-Analysis-Texts-Mathematics/dp/0387948414},
year = {1996}
}
@article{Blanco-Claraco2021a,
abstract = {An arbitrary rigid transformation in $\backslashbackslashmathbf{\{}\backslash{\{}{\}}SE{\{}\backslash{\}}{\}}(3)$ can be separated into two parts, namely, a translation and a rigid rotation. This technical report reviews, under a unifying viewpoint, three common alternatives to representing the rotation part: sets of three (yaw-pitch-roll) Euler angles, orthogonal rotation matrices from $\backslashbackslashmathbf{\{}\backslash{\{}{\}}SO{\{}\backslash{\}}{\}}(3)$ and quaternions. It will be described: (i) the equivalence between these representations and the formulas for transforming one to each other (in all cases considering the translational and rotational parts as a whole), (ii) how to compose poses with poses and poses with points in each representation and (iii) how the uncertainty of the poses (when modeled as Gaussian distributions) is affected by these transformations and compositions. Some brief notes are also given about the Jacobians required to implement least-squares optimization on manifolds, an very promising approach in recent engineering literature. The text reflects which MRPT C++ library functions implement each of the described algorithms. All formulas and their implementation have been thoroughly validated by means of unit testing and numerical estimation of the Jacobians},
archivePrefix = {arXiv},
arxivId = {2103.15980},
author = {Blanco-Claraco, Jos{\'{e}} Luis},
eprint = {2103.15980},
number = {3},
title = {{A tutorial on $\backslashbackslashmathbf{\{}\backslash{\{}{\}}SE{\{}\backslash{\}}{\}}(3)$ transformation parameterizations and on-manifold optimization}},
url = {http://arxiv.org/abs/2103.15980},
year = {2021}
}
@article{Jiao2018a,
abstract = {This paper proposes a new framework to solve the problem of monocular visual odometry, called MagicVO . Based on Convolutional Neural Network (CNN) and Bi-directional LSTM (Bi-LSTM), MagicVO outputs a 6-DoF absolute-scale pose at each position of the camera with a sequence of continuous monocular images as input. It not only utilizes the outstanding performance of CNN in image feature processing to extract the rich features of image frames fully but also learns the geometric relationship from image sequences pre and post through Bi-LSTM to get a more accurate prediction. A pipeline of the MagicVO is shown in Fig. 1. The MagicVO system is end-to-end, and the results of experiments on the KITTI dataset and the ETH-asl cla dataset show that MagicVO has a better performance than traditional visual odometry (VO) systems in the accuracy of pose and the generalization ability.},
archivePrefix = {arXiv},
arxivId = {1811.10964},
author = {Jiao, Jian Jichao and Jiao, Jian Jichao and Mo, Yaokai and Liu, Weilun and Deng, Zhongliang},
eprint = {1811.10964},
pages = {1--9},
title = {{MagicVO: End-to-End Monocular Visual Odometry through Deep Bi-directional Recurrent Convolutional Neural Network}},
url = {http://arxiv.org/abs/1811.10964},
year = {2018}
}
@article{Lin2022a,
abstract = {Simultaneous localization and mapping (SLAM) are crucial for autonomous robots (e.g., self-driving cars, autonomous drones), 3D mapping systems, and AR/VR applications. This work proposed a novel LiDAR-inertial-visual fusion framework termed R$^3$LIVE++ to achieve robust and accurate state estimation while simultaneously reconstructing the radiance map on the fly. R$^3$LIVE++ consists of a LiDAR-inertial odometry (LIO) and a visual-inertial odometry (VIO), both running in real-time. The LIO subsystem utilizes the measurements from a LiDAR for reconstructing the geometric structure (i.e., the positions of 3D points), while the VIO subsystem simultaneously recovers the radiance information of the geometric structure from the input images. R$^3$LIVE++ is developed based on R$^3$LIVE and further improves the accuracy in localization and mapping by accounting for the camera photometric calibration (e.g., non-linear response function and lens vignetting) and the online estimation of camera exposure time. We conduct more extensive experiments on both public and our private datasets to compare our proposed system against other state-of-the-art SLAM systems. Quantitative and qualitative results show that our proposed system has significant improvements over others in both accuracy and robustness. In addition, to demonstrate the extendability of our work, {we developed several applications based on our reconstructed radiance maps, such as high dynamic range (HDR) imaging, virtual environment exploration, and 3D video gaming.} Lastly, to share our findings and make contributions to the community, we make our codes, hardware design, and dataset publicly available on our Github: github.com/hku-mars/r3live},
archivePrefix = {arXiv},
arxivId = {2209.03666},
author = {Lin, Jiarong and Zhang, Fu},
eprint = {2209.03666},
title = {{R$^3$LIVE++: A Robust, Real-time, Radiance reconstruction package with a tightly-coupled LiDAR-Inertial-Visual state Estimator}},
url = {http://arxiv.org/abs/2209.03666},
year = {2022}
}
@article{Darcet2023,
abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
archivePrefix = {arXiv},
arxivId = {2309.16588},
author = {Darcet, Timoth{\'{e}}e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
eprint = {2309.16588},
pages = {1--16},
title = {{Vision Transformers Need Registers}},
url = {http://arxiv.org/abs/2309.16588},
year = {2023}
}
@misc{Shuster1993b,
abstract = {A survey of the attitude representations is given in a single consistent notation and set of conventions. The relations between the various representations of the attitude, and the kinematic equations are given completely. The transformations connecting different attitude covariance representations are presented for the case where the errors in the attitude are sufficiently small that they can be represented by an infinitesimal rotation. Examples of the use of each representation are presented and some historical notes provided.},
author = {Shuster, Malcolm D.},
booktitle = {Journal of the Astronautical Sciences},
issn = {00219142},
number = {4},
pages = {439--517},
title = {{Survey of attitude representations}},
volume = {41},
year = {1993}
}
@article{Sola2012,
author = {Sola, Joan},
journal = {Laboratoire dAnalyse et dArchitecture des Systemes-Centre national de la recherche scientifique (LAAS-CNRS), Toulouse, France, Tech. Re},
pages = {6},
title = {{so3,quaternions,rotation- Quaternion kinematics for the error-state KF}},
url = {http://www.billion.uk.com/downloads/user manual/annex_m.pdf},
year = {2012}
}
@article{Ouabi2022,
abstract = {The inspection of sizeable plate-based metal structures such as storage tanks or marine vessel hulls is a significant stake in the industry, which necessitates reliable and time-efficient solutions. Although Lamb waves have been identified as a promising solution for long-range non-destructive testing, and despite the substantial progress made in autonomous navigation and environment sensing, a Lamb-wave-based robotic system for extensive structure monitoring is still lacking. Following previous work on ultrasonic Simultaneous Localization and Mapping (SLAM), we introduce a method to achieve plate geometry inference without prior knowledge of the material propagation properties, which may be lacking during a practical inspection task in challenging and outdoor environments. Our approach combines focalization to adjust the propagation model parameters and beamforming to infer the plate boundaries location by relying directly on acoustic measurements acquired along the mobile unit trajectory. For each candidate model, the focusing ability of the corresponding beamformer is assessed over high-pass filtered beamforming maps to further improve the robustness of the plate geometry estimates. We then recover the optimal space-domain beamformer through a simulated annealing optimization process. We evaluate our method on three sets of experimental data acquired in different conditions and show that accurate plate geometry inference can be achieved without any prior propagation model. Finally, the results show that the optimal beamformer outperforms the beamformer resulting from the predetermined propagation model in non-nominal acquisition conditions.},
archivePrefix = {arXiv},
arxivId = {arXiv:2107.05842v1},
author = {Ouabi, Othmane-Latif and Pomarede, Pascal and Declercq, Nico and Zeghidour, Neil and Geist, Matthieu and Pradalier, C{\'{e}}dric and Declercq, Nico F and {Edric Pradalier}, C ´},
doi = {10.1177/ToBeAssigned},
eprint = {arXiv:2107.05842v1},
file = {:C\:/Users/wuch2/Nutstore/1/papers/mins.pdf:pdf},
journal = {Ultrasonics},
keywords = {Acoustic mapping,Helmholtz equation,Lamb waves,Model learning,Optimal beamforming},
number = {...},
pages = {106705},
title = {{Learning the Propagation Properties of Plate-like Structures for Lamb Wave-based Mapping Learn-ing the Propagation Properties of Plate-like Structures for Lamb Wave-based Mapping Learning the Propagation Properties of Plate-like Structures for Lamb Wave-b}},
volume = {...},
year = {2022}
}
@article{Li2022,
abstract = {In this research, we propose a new 3D object detector with a trustworthy depth estimation, dubbed BEVDepth, for camera-based Bird's-Eye-View (BEV) 3D object detection. By a thorough analysis of recent approaches, we discover that the depth estimation is implicitly learned without camera information, making it the de-facto fake-depth for creating the following pseudo point cloud. BEVDepth gets explicit depth supervision utilizing encoded intrinsic and extrinsic parameters. A depth correction sub-network is further introduced to counteract projecting-induced disturbances in depth ground truth. To reduce the speed bottleneck while projecting features from image-view into BEV using estimated depth, a quick view-transform operation is also proposed. Besides, our BEVDepth can be easily extended with input from multi-frame. Without any bells and whistles, BEVDepth achieves the new state-of-the-art 60.0% NDS on the challenging nuScenes test set while maintaining high efficiency. For the first time, the performance gap between the camera and LiDAR is largely reduced within 10% NDS.},
archivePrefix = {arXiv},
arxivId = {2206.10092},
author = {Li, Yinhao and Ge, Zheng and Yu, Guanyi and Yang, Jinrong and Wang, Zengran and Shi, Yukang and Sun, Jianjian and Li, Zeming},
eprint = {2206.10092},
title = {{BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection}},
url = {http://arxiv.org/abs/2206.10092},
year = {2022}
}
@article{Forster2014b,
author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
journal = {Icra14},
title = {{ICRA14_Forster}},
year = {2014}
}
@article{CamposMartinez2020,
abstract = {This paper presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a feature-based tightly-integrated visual-inertial SLAM system that fully relies on Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization phase. The result is a system that operates robustly in real time, in small and large, indoor and outdoor environments, and is 2 to 5 times more accurate than previous approaches. The second main novelty is a multiple map system that relies on a new place recognition method with improved recall. Thanks to it, ORB-SLAM3 is able to survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting mapped areas. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information. This allows to include in bundle adjustment co-visible keyframes, that provide high parallax observations boosting accuracy, even if they are widely separated in time or if they come from a previous mapping session. Our experiments show that, in all sensor configurations, ORBSLAM3 is as robust as the best systems available in the literature, and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting representative of AR/VR scenarios. For the benefit of the community we make public the source code.},
archivePrefix = {arXiv},
arxivId = {2007.11898},
author = {{Campos Mart{\'{i}}nez}, Carlos and Elvira, Richard and {G{\'{o}}mez Rodr{\'{i}}guez}, Juan J. and Montiel, Jos{\'{e}} M.M. and Tard{\'{o}}s, Juan D. and Campos, Carlos and Elvira, Richard and Juan, J G},
eprint = {2007.11898},
issn = {23318422},
journal = {arXiv},
pages = {1--15},
title = {{ORB-SLAM3: An accurate Open-source library for visual, Visual-inertial and Multi-map SLAM}},
year = {2020}
}
@article{Weidmann2011,
abstract = {Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Es- tablished leaders in the field are the SIFT and SURF al- gorithms which exhibit great performance under a variety of image transformations, with SURF in particular consid- ered as the most computationally efficient amongst the high- performance methods to date. In this paper we propose BRISK1, a novel method for keypoint detection, description and matching. A compre- hensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art al- gorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.},
author = {Waraich and Weidmann},
isbn = {8610828378018},
journal = {BRISK Binary Robust Invariant Scalable Keypoints},
number = {3},
pages = {12--19},
title = {{Research Collection}},
url = {https://doi.org/10.3929/ethz-a-010025751},
volume = {15},
year = {2011}
}
@article{Li2019a,
author = {Li, Kun},
title = {{Dso代码解读}},
year = {2019}
}
@article{Li2013b,
abstract = {In this report, we perform a rigorous analysis of EKF-based visual-inertial odometry (VIO) and present a method for improving its performance. Specifically, we examine the properties of EKF-based VIO, and show that the standard way of computing Jacobians in the filter inevitably causes inconsistency and loss of accuracy. This result is derived based on an observability analysis of the EKF's linearized system model, which proves that the yaw erroneously appears to be observable. In order to address this problem, we propose modifications to the multi-state constraint Kalman filter (MSCKF) algorithm [1], which ensure the correct observability properties without incurring additional computational cost. Extensive simulation tests and real-world experiments demonstrate that the modified MSCKF algorithm outperforms competing methods, both in terms of consistency and accuracy.},
author = {Li, Mingyang and Mourikis, Anastasios I},
journal = {International Jouranl of Roboticcs Research},
pages = {690--711},
title = {{Consistency of EKF-Based Visual-Inertial Odometry}},
volume = {32},
year = {2013}
}
@article{Peng,
archivePrefix = {arXiv},
arxivId = {arXiv:2203.04050v1},
author = {Peng, Lang and Chen, Zhirong and Fu, Zhangjie and Liang, Pengpeng and Cheng, Erkang},
eprint = {arXiv:2203.04050v1},
title = {{BEVSegFormer : Bird ' s Eye View Semantic Segmentation From}}
}
@article{Hartleya,
author = {Hartley, Richard and Zisserman, Andrew},
title = {多视图几何}
}
@article{PaulDebevec1998a,
abstract = {We present a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equip- ment. In our method, multiple photographs of the scene are taken with different amounts of exposure. Our algorithm uses these dif- ferently exposed photographs to recover the response function of the imaging process, up to factor of scale, using the assumption of reci- procity. With the known response function, the algorithm can fuse themultiple photographs into a single, high dynamic range radiance map whose pixel values are proportional to the true radiance values in the scene. We demonstrate our method on images acquired with both photochemical and digital imaging processes. We discuss how this work is applicable in many areas of computer graphics involv- ing digitized photographs, including image-based modeling, image compositing, and image processing. Lastly, we demonstrate a few applications of having high dynamic range radiance maps, such as synthesizing realistic motion blur and simulating the response of the human visual system.},
author = {{Paul Debevec}, Jitendra Malik},
journal = {ACM Transactions on Graphics},
keywords = {High Dynamic Range,SVD},
title = {{Recovering High Dynamic Range Radiance Maps from Photographs Paul E . Debevec}},
year = {1998}
}
@article{Huang2019,
abstract = {Simultaneous Localization and Mapping (SLAM) achieves the purpose of simultaneous positioning and map construction based on self-perception. The paper makes an overview in SLAM including Lidar SLAM, visual SLAM, and their fusion. For Lidar or visual SLAM, the survey illustrates the basic type and product of sensors, open source system in sort and history, deep learning embedded, the challenge and future. Additionally, visual inertial odometry is supplemented. For Lidar and visual fused SLAM, the paper highlights the multi-sensors calibration, the fusion in hardware, data, task layer. The open question and forward thinking with an envision in 6G wireless networks end the paper. The contributions of this paper can be summarized as follows: the paper provides a high quality and full-scale overview in SLAM. It's very friendly for new researchers to hold the development of SLAM and learn it very obviously. Also, the paper can be considered as a dictionary for experienced researchers to search and find new interesting orientation.},
archivePrefix = {arXiv},
arxivId = {1909.05214},
author = {Huang, Baichuan and Zhao, Jun and Liu, Jingbin},
eprint = {1909.05214},
pages = {1--17},
title = {{A Survey of Simultaneous Localization and Mapping with an Envision in 6G Wireless Networks}},
url = {http://arxiv.org/abs/1909.05214},
year = {2019}
}
@article{Engel2016a,
abstract = {We present a dataset for evaluating the tracking accuracy of monocular visual odometry and SLAM methods. It contains 50 real-world sequences comprising more than 100 minutes of video, recorded across dozens of different environments -- ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position. This allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated. We provide exposure times for each frame as reported by the sensor, the camera response function, and dense lens attenuation factors. We also propose a novel, simple approach to non-parametric vignette calibration, which requires minimal set-up and is easy to reproduce. Finally, we thoroughly evaluate two existing methods (ORB-SLAM and DSO) on the dataset, including an analysis of the effect of image resolution, camera field of view, and the camera motion direction.},
archivePrefix = {arXiv},
arxivId = {1607.02555},
author = {Engel, Jakob and Usenko, Vladyslav and Cremers, Daniel},
eprint = {1607.02555},
title = {{A Photometrically Calibrated Benchmark For Monocular Visual Odometry}},
url = {http://arxiv.org/abs/1607.02555},
year = {2016}
}
@article{Liu2017a,
abstract = {In this paper, we present RKD-SLAM, a robust keyframe-based dense SLAM approach for an RGB-D camera that can robustly handle fast motion and dense loop closure, and run without time limitation in a moderate size scene. It not only can be used to scan high-quality 3D models, but also can satisfy the demand of VR and AR applications. First, we combine color and depth information to construct a very fast keyframe-based tracking method on a CPU, which can work robustly in challenging cases (e.g.$\sim$fast camera motion and complex loops). For reducing accumulation error, we also introduce a very efficient incremental bundle adjustment (BA) algorithm, which can greatly save unnecessary computation and perform local and global BA in a unified optimization framework. An efficient keyframe-based depth representation and fusion method is proposed to generate and timely update the dense 3D surface with online correction according to the refined camera poses of keyframes through BA. The experimental results and comparisons on a variety of challenging datasets and TUM RGB-D benchmark demonstrate the effectiveness of the proposed system.},
archivePrefix = {arXiv},
arxivId = {1711.05166},
author = {Liu, Haomin and Li, Chen and Chen, Guojun and Zhang, Guofeng and Kaess, Michael and Bao, Hujun},
eprint = {1711.05166},
pages = {1--12},
title = {{Robust Keyframe-based Dense SLAM with an RGB-D Camera}},
url = {http://arxiv.org/abs/1711.05166},
year = {2017}
}
@article{Trawny2005b,
abstract = {The quaternion is generally defined as ¯ q = q 4 + q 1 i + q 2 j + q 3 k (1) where i, j, and k are hyperimaginary numbers satisfying i 2 = −1 , j 2 = −1 , k 2 = −1 , −ij = ji = k , − jk = kj = i , − ki = ik = j (2) Note that this does not correspond to the Hamilton notation. It rather is a convention resulting in multiplications of quaternions in " natural order " (see also section 1.4 and [1, p. 473]). This is in accordance with the JPL Proposed Standard Conventions [2]. The quantity q 4 is the real or scalar part of the quaternion, and q 1 i + q 2 j + q 3 k is the imaginary or vector part. The quaternion can therefore also be written in a four-dimensional column matrix ¯ q, given by ¯ q =},
author = {Trawny, Nikolas and Roumeliotis, Stergios I},
journal = {Engineering},
number = {612},
title = {{Indirect Kalman Filter for 3D Attitude Estimation A Tutorial for Quaternion Algebra Multiple Autonomous Robotic Systems Laboratory Technical Report Number 2005-002 , Rev . 57}},
url = {http://www.cs.umn.edu},
year = {2005}
}
@article{Wang2023,
abstract = {This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF) for novel view synthesis, which enables arbitrary input camera trajectories and only costs a few minutes for training. Existing fast grid-based NeRF training frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed for bounded scenes and rely on space warping to handle unbounded scenes. Existing two widely-used space-warping methods are only designed for the forward-facing trajectory or the 360-degree object-centric trajectory but cannot process arbitrary trajectories. In this paper, we delve deep into the mechanism of space warping to handle unbounded scenes. Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework. Extensive experiments demonstrate that F2-NeRF is able to use the same perspective warping to render high-quality images on two standard datasets and a new free trajectory dataset collected by us. Project page: https://totoro97.github.io/projects/f2-nerf.},
archivePrefix = {arXiv},
arxivId = {2303.15951},
author = {Wang, Peng and Liu, Yuan and Chen, Zhaoxi and Liu, Lingjie and Liu, Ziwei and Komura, Taku and Theobalt, Christian and Wang, Wenping},
eprint = {2303.15951},
pages = {1--17},
title = {{F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories}},
url = {http://arxiv.org/abs/2303.15951},
year = {2023}
}
@article{Odometry2011,
author = {Odometry, Visual-inertial and Li, Mingyang and Mourikis, Anastasios I},
title = {{High-Precision, Consistent EKF-based Visual-Inertial Odometry msckf2.0}},
year = {2011}
}
@article{Kirillov2023b,
abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
archivePrefix = {arXiv},
arxivId = {2304.02643},
author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'{a}}r, Piotr and Girshick, Ross},
eprint = {2304.02643},
title = {{Segment Anything}},
url = {http://arxiv.org/abs/2304.02643},
year = {2023}
}
@misc{Unknown,
author = {秦永元},
title = {惯性导航_秦永元}
}
@article{Maddocks2019a,
author = {Maddocks, Prof John},
number = {1},
pages = {3--4},
title = {{Properties of skew symmetric matrices}},
year = {2019}
}
@article{Barron2021,
abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
archivePrefix = {arXiv},
arxivId = {2103.13415},
author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
eprint = {2103.13415},
title = {{Mip-NeRF(with supplement)}},
url = {http://arxiv.org/abs/2103.13415},
year = {2021}
}
@article{Kaess2008c,
abstract = {We present incremental smoothing and mapping (iSAM), a novel approach to the simultaneous localization and mapping problem that is based on fast incremental matrix factorization. iSAM provides an efficient and exact solution by updating a QR factorization of the naturally sparse smoothing information matrix, therefore recalculating only the matrix entries that actually change. iSAM is efficient even for robot trajectories with many loops as it avoids unnecessary fill-in in the factor matrix by periodic variable reordering. Also, to enable data association in real-time, we provide efficient algorithms to access the estimation uncertainties of interest based on the factored information matrix. We systematically evaluate the different components of iSAM as well as the overall algorithm using various simulated and real-world datasets for both landmark and pose-only settings.},
author = {Kaess, Michael},
keywords = {Index Terms-Data association,localization,mapping,mobile robots,nonlinear estimation,simultaneous localization and map-ping (SLAM),smoothing},
number = {December},
pages = {1365--1378},
title = {{Incremental Smoothing and Mapping ({iSAM}) Library}},
volume = {24},
year = {2008}
}
@article{Rosinol2022a,
abstract = {We propose a novel geometric and photometric 3D mapping pipeline for accurate and real-time scene reconstruction from monocular images. To achieve this, we leverage recent advances in dense monocular SLAM and real-time hierarchical volumetric neural radiance fields. Our insight is that dense monocular SLAM provides the right information to fit a neural radiance field of the scene in real-time, by providing accurate pose estimates and depth-maps with associated uncertainty. With our proposed uncertainty-based depth loss, we achieve not only good photometric accuracy, but also great geometric accuracy. In fact, our proposed pipeline achieves better geometric and photometric accuracy than competing approaches (up to 179% better PSNR and 86% better L1 depth), while working in real-time and using only monocular images.},
archivePrefix = {arXiv},
arxivId = {2210.13641},
author = {Rosinol, Antoni and Leonard, John J. and Carlone, Luca},
eprint = {2210.13641},
title = {{NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields}},
url = {http://arxiv.org/abs/2210.13641},
year = {2022}
}
@article{Eigen2014a,
abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
archivePrefix = {arXiv},
arxivId = {1406.2283},
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
eprint = {1406.2283},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {2366--2374},
title = {{Depth map prediction from a single image using a multi-scale deep network}},
volume = {3},
year = {2014}
}
@article{Losa,
author = {Los, Unidad Metodolog{\'{i}}a D E Conocimiento D E},
title = {{No 主観的健康感を中心とした在宅高齢者における 健康関連指標に関する共分散構造分析Title}},
url = {https://arxiv.org/abs/2304.05316 https://arxiv.org/pdf/2007.12140v5.pdf}
}
@article{Eckenhoff2018,
abstract = {In this paper we propose a new continuous preintegration theory for graph-based sensor fusion with an inertial measurement unit (IMU) and a camera (or other aiding sensors). Rather than using discrete sampling of the measurement dynamics as in current methods, we analytically derive the closed-form solutions to the preintegration equations, yielding improved accuracy in state estimation. We advocate two new different inertial models for preintegration: (i) the model that assumes piecewise constant measurements, and (ii) the model that assumes piecewise constant local true acceleration. We show through extensive Monte-Carlo simulations the effect that the choice of preintegration model has on estimation performance. To validate the proposed preintegration theory, we develop both direct and indirect visual-inertial navigation systems (VINS) that leverage our preintegration. In the first, within a tightly-coupled, sliding-window optimization framework, we jointly estimate the features in the window and the IMU states while performing marginalization to bound the computational cost. In the second, we loosely couple the IMU preintegration with a direct image alignment that estimates relative camera motion by minimizing the photometric errors (i.e., raw image intensity difference), allowing for efficient and informative loop closures. Both systems are extensively tested in real-world experiments and are shown to offer competitive performance to state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1805.02774},
author = {Eckenhoff, Kevin and Geneva, Patrick and Huang, Guoquan},
eprint = {1805.02774},
isbn = {9781509037612},
journal = {arXIv},
title = {{Continuous Preintegration Theory for Graph-based Visual-Inertial Navigation}},
url = {http://arxiv.org/abs/1805.02774},
year = {2018}
}
@article{Pumarola2020,
abstract = {While the performance of the state-of-the-art point-based VSLAM (vision simultaneous localization and mapping) systems in well textured sequences is impressive, their performance in poorly textured situations is not satisfactory enough. A sensible alternative or addition is to consider lines. In this paper, we propose a novel line-assisted point-based VSLAM algorithm (LAP-SLAM). Our algorithm uses lines without descriptor matching, and the lines are used to assist the computation conducted by points. To the best of our knowledge, this paper proposes a new way to include line information in VSLAM. The basic idea is to use the collinear relationship of points to optimize the current point-based VSLAM algorithm. In LAP-SLAM, we propose a practical algorithm to match lines and compute the collinear relationship of points, a line-assisted bundle adjustment approach and a modified perspective-n-point (PnP) approach. We built our system based on the architecture and pipeline of ORB-SLAM. We evaluate the proposed method on a diverse range of indoor sequences in the TUM dataset and compare with point-based and point-line-based methods. The results show that the accuracy of our algorithm is close to point-line-based VSLAM systems with a much faster speed.},
archivePrefix = {arXiv},
arxivId = {2009.09972},
author = {Pumarola, Albert and Vakhitov, Alexander and Agudo, Antonio and Sanfeliu, Alberto and Moreno-Noguer, Francese and Csurka, Gabriela and Kraus, Martin and Mestetskiy, Leonid and Richard, Paul and Braz, Jos{\'{e}} and Zhang, Guoxuan and Suh, Il Hong and Heidenreich, Toni and Spehr, Jens and Stiller, Christoph and Zhang, Fukai and Rui, Ting and Yang, Chengsong and Shi, Jianjun and Wang, Qiuyuan and Yan, Zike and Wang, Junqiu and Xue, Fei and Ma, Wei and Zha, Hongbin and Im, Gyubeom and Jeong, Jinyong and Cho, Younggun and Kim, Ayoung and He, Yijia and Zhang, Hong and Lee, Junesuk and Park, Soon-yong and Qin, Tong and Li, Peiliang and Shen, Shaojie and Yang, Yulin and Geneva, Patrick and Eckenhoff, Kevin and Huang, Guoquan and Suh, Jongsang and Choi, Eric Yongkeun and Borrelli, Francesco},
eprint = {2009.09972},
isbn = {9781467365956},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Bundle adjustment,Collinear relationship,High-curved track,Line-assisted,Monocular VSLAM,iterative closest point,stochastic gradient descent optimization,vision-based simulta-neous localization and mappin},
number = {2},
pages = {1--17},
title = {{PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines Albert}},
url = {http://arxiv.org/abs/2009.09972},
volume = {8},
year = {2020}
}
@article{Vijayanarasimhan2017a,
abstract = {We propose SfM-Net, a geometry-aware neural network for motion estimation in videos that decomposes frame-to-frame pixel motion in terms of scene and object depth, camera motion and 3D object rotations and translations. Given a sequence of frames, SfM-Net predicts depth, segmentation, camera and rigid object motions, converts those into a dense frame-to-frame motion field (optical flow), differentiably warps frames in time to match pixels and back-propagates. The model can be trained with various degrees of supervision: 1) self-supervised by the re-projection photometric error (completely unsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by depth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth estimates and successfully estimates frame-to-frame camera rotations and translations. It often successfully segments the moving objects in the scene, even though such supervision is never provided.},
archivePrefix = {arXiv},
arxivId = {1704.07804},
author = {Vijayanarasimhan, Sudheendra and Ricco, Susanna and Schmid, Cordelia and Sukthankar, Rahul and Fragkiadaki, Katerina},
eprint = {1704.07804},
title = {{SfM-Net: Learning of Structure and Motion from Video}},
url = {http://arxiv.org/abs/1704.07804},
year = {2017}
}
@article{Yang2024,
abstract = {This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data ($\sim$62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.},
archivePrefix = {arXiv},
arxivId = {2401.10891},
author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
eprint = {2401.10891},
title = {{Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}},
url = {http://arxiv.org/abs/2401.10891},
year = {2024}
}
@article{Gao2018,
author = {Gao, Xiang},
pages = {1--9},
title = {{Notes on DSO}},
year = {2018}
}
@article{Mitra2018,
abstract = {We propose a new dataset for learning local image descriptors which can be used for significantly improved patch matching. Our proposed dataset consists of an order of magnitude more number of scenes, images, and positive and negative correspondences compared to the currently available Multi-View Stereo (MVS) dataset from Brown et al. The new dataset also has better coverage of the overall viewpoint, scale, and lighting changes in comparison to the MVS dataset. Our dataset also provides supplementary information like RGB patches with scale and rotations values, and intrinsic and extrinsic camera parameters which as shown later can be used to customize training data as per application. We train an existing state-of-the-art model on our dataset and evaluate on publicly available benchmarks such as HPatches dataset and Strecha et al.$\$cite{strecha} to quantify the image descriptor performance. Experimental evaluations show that the descriptors trained using our proposed dataset outperform the current state-of-the-art descriptors trained on MVS by 8%, 4% and 10% on matching, verification and retrieval tasks respectively on the HPatches dataset. Similarly on the Strecha dataset, we see an improvement of 3-5% for the matching task in non-planar scenes.},
archivePrefix = {arXiv},
arxivId = {1801.01466},
author = {Mitra, Rahul and Doiphode, Nehal and Gautam, Utkarsh and Narayan, Sanath and Ahmed, Shuaib and Chandran, Sharat and Jain, Arjun},
eprint = {1801.01466},
title = {{A Large Dataset for Improving Patch Matching}},
url = {http://arxiv.org/abs/1801.01466},
year = {2018}
}
@article{Vogiatzis2011,
author = {Vogiatzis, George and {Hern{\'{a}}ndez, CarlosVogiatzis}, George and Hern{\'{a}}ndez, Carlos},
number = {8},
pages = {1--3},
title = {{Supplementary matterial Parametric approximation to posterior}},
year = {2011}
}
@article{DeTone2017,
abstract = {We present a point tracking system powered by two deep convolutional neural networks. The first network, MagicPoint, operates on single images and extracts salient 2D points. The extracted points are "SLAM-ready" because they are by design isolated and well-distributed throughout the image. We compare this network against classical point detectors and discover a significant performance gap in the presence of image noise. As transformation estimation is more simple when the detected points are geometrically stable, we designed a second network, MagicWarp, which operates on pairs of point images (outputs of MagicPoint), and estimates the homography that relates the inputs. This transformation engine differs from traditional approaches because it does not use local point descriptors, only point locations. Both networks are trained with simple synthetic data, alleviating the requirement of expensive external camera ground truthing and advanced graphics rendering pipelines. The system is fast and lean, easily running 30+ FPS on a single CPU.},
archivePrefix = {arXiv},
arxivId = {1707.07410},
author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
eprint = {1707.07410},
keywords = {augmented reality,deep learning,geometry,slam,tracking},
title = {{Toward Geometric Deep SLAM}},
url = {http://arxiv.org/abs/1707.07410},
year = {2017}
}
@article{Lee2011,
author = {Lee, Lecturer Che-rung},
pages = {2--7},
title = {{Lecture Notes 6 : Givens roration Givens roration}},
year = {2011}
}
@article{Guo2022,
abstract = {We present SegNeXt, a simple convolutional network architecture for semantic segmentation. Recent transformer-based models have dominated the field of semantic segmentation due to the efficiency of self-attention in encoding spatial information. In this paper, we show that convolutional attention is a more efficient and effective way to encode contextual information than the self-attention mechanism in transformers. By re-examining the characteristics owned by successful segmentation models, we discover several key components leading to the performance improvement of segmentation models. This motivates us to design a novel convolutional attention network that uses cheap convolutional operations. Without bells and whistles, our SegNeXt significantly improves the performance of previous state-of-the-art methods on popular benchmarks, including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal Context, and iSAID. Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and achieves 90.6% mIoU on the Pascal VOC 2012 test leaderboard using only 1/10 parameters of it. On average, SegNeXt achieves about 2.0% mIoU improvements compared to the state-of-the-art methods on the ADE20K datasets with the same or fewer computations. Code is available at https://github.com/uyzhang/JSeg (Jittor) and https://github.com/Visual-Attention-Network/SegNeXt (Pytorch).},
archivePrefix = {arXiv},
arxivId = {2209.08575},
author = {Guo, Meng-Hao and Lu, Cheng-Ze and Hou, Qibin and Liu, Zhengning and Cheng, Ming-Ming and Hu, Shi-Min},
eprint = {2209.08575},
number = {NeurIPS},
pages = {1--15},
title = {{SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation}},
url = {http://arxiv.org/abs/2209.08575},
year = {2022}
}
@article{Dosovitskiy2020a,
abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
archivePrefix = {arXiv},
arxivId = {2010.11929},
author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
eprint = {2010.11929},
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
url = {http://arxiv.org/abs/2010.11929},
year = {2020}
}
@article{Bartoli2006,
author = {Bartoli, Adrien and Sturm, Peter and Bartoli, Adrien and Sturm, Peter and Lines, Structure-from-motion Using and Bartoli, Adrien and Sturm, Peter},
title = {{Structure-From-Motion Using Lines : Representation , Triangulation and Bundle Adjustment To cite this version : HAL Id : hal-00092589 Structure-From-Motion Using Lines : Representation , Triangulation and Bundle Adjustment}},
year = {2006}
}
@book{·2019a,
abstract = {{\ldots}Akses terbuka (open access), memberi keleluasaan kepada siapapun yang membutuhkan informasi khususnya melalui sumber daya {\ldots}Dalam akses terbuka (open access), informasi dengan teknologi digital ada kebebasan untuk mengakses bagi siapa saja yang membutuhkan {\ldots}},
author = {毗湿奴{\textperiodcentered}布拉马尼亚著},
booktitle = {人民邮电出版社},
isbn = {9781617297120},
title = {{PyTorch深度学习}},
year = {2019}
}
@article{Guo2022,
abstract = {We present SegNeXt, a simple convolutional network architecture for semantic segmentation. Recent transformer-based models have dominated the field of semantic segmentation due to the efficiency of self-attention in encoding spatial information. In this paper, we show that convolutional attention is a more efficient and effective way to encode contextual information than the self-attention mechanism in transformers. By re-examining the characteristics owned by successful segmentation models, we discover several key components leading to the performance improvement of segmentation models. This motivates us to design a novel convolutional attention network that uses cheap convolutional operations. Without bells and whistles, our SegNeXt significantly improves the performance of previous state-of-the-art methods on popular benchmarks, including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal Context, and iSAID. Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and achieves 90.6% mIoU on the Pascal VOC 2012 test leaderboard using only 1/10 parameters of it. On average, SegNeXt achieves about 2.0% mIoU improvements compared to the state-of-the-art methods on the ADE20K datasets with the same or fewer computations. Code is available at https://github.com/uyzhang/JSeg (Jittor) and https://github.com/Visual-Attention-Network/SegNeXt (Pytorch).},
archivePrefix = {arXiv},
arxivId = {2209.08575},
author = {Guo, Meng-Hao and Lu, Cheng-Ze and Hou, Qibin and Liu, Zhengning and Cheng, Ming-Ming and Hu, Shi-Min},
eprint = {2209.08575},
number = {NeurIPS},
pages = {1--15},
title = {{SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation}},
url = {http://arxiv.org/abs/2209.08575},
year = {2022}
}
@article{Notes2018,
author = {Notes, Com S},
pages = {1--11},
title = {{Plucker Coordinates for Lines in the Space}},
year = {2018}
}
@article{Marchal2006b,
abstract = {The fusion of visual and inertial cues has become popular in robotics due to the complementary nature of the two sensing modalities. While most fusion strategies to date rely on filtering schemes, the visual robotics community has recently turned to non-linear optimization approaches for tasks such as visual Simultaneous Localization And Mapping (SLAM), following the discovery that this comes with signifi- cant advantages in quality of performance and computational complexity. Following this trend, we present a novel approach to tightly integrate visual measurements with readings from an Inertial Measurement Unit (IMU) in SLAM. An IMU error term is integrated with the landmark reprojection error in a fully probabilistic manner, resulting to a joint non-linear cost function to be optimized. Employing the powerful concept of ‘keyframes' we partially marginalize old states to maintain a bounded-sized optimization window, ensuring real-time opera- tion. Comparing against both vision-only and loosely-coupled visual-inertial algorithms, our experiments confirm the benefits of tight fusion in terms of accuracy and robustness.},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Marchal, An},
eprint = {1502.00956},
isbn = {9789810739379},
issn = {0278-3649},
number = {september},
pages = {1--30},
pmid = {21736739},
title = {{Keyframe-Based Visual-Inertial SLAM Using Nonlinear Optimization}},
year = {2006}
}
@article{Blanco-Claraco2021a,
abstract = {An arbitrary rigid transformation in $\mathbf{SE}(3)$ can be separated into two parts, namely, a translation and a rigid rotation. This technical report reviews, under a unifying viewpoint, three common alternatives to representing the rotation part: sets of three (yaw-pitch-roll) Euler angles, orthogonal rotation matrices from $\mathbf{SO}(3)$ and quaternions. It will be described: (i) the equivalence between these representations and the formulas for transforming one to each other (in all cases considering the translational and rotational parts as a whole), (ii) how to compose poses with poses and poses with points in each representation and (iii) how the uncertainty of the poses (when modeled as Gaussian distributions) is affected by these transformations and compositions. Some brief notes are also given about the Jacobians required to implement least-squares optimization on manifolds, an very promising approach in recent engineering literature. The text reflects which MRPT C++ library functions implement each of the described algorithms. All formulas and their implementation have been thoroughly validated by means of unit testing and numerical estimation of the Jacobians},
archivePrefix = {arXiv},
arxivId = {2103.15980},
author = {Blanco-Claraco, Jos{\'{e}} Luis},
eprint = {2103.15980},
number = {3},
title = {{A tutorial on $\mathbf{SE}(3)$ transformation parameterizations and on-manifold optimization}},
url = {http://arxiv.org/abs/2103.15980},
year = {2021}
}
@article{Xie2021,
abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5× smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.},
archivePrefix = {arXiv},
arxivId = {2105.15203},
author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
eprint = {2105.15203},
isbn = {9781713845393},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {12077--12090},
title = {{SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers}},
volume = {15},
year = {2021}
}
@article{Hesch2012b,
abstract = {In this technical report, we study estimator inconsistency in Vision-aided Inertial Navigation Systems (VINS) from a standpoint of system observability. We postulate that a leading cause of inconsistency is the gain of spurious information along unobservable directions, resulting in smaller uncertainties, larger estimation errors, and divergence. We support our claim with an analytical study of the Observability Gramian, along with its right nullspace, which constitutes the basis of the unobservable directions of the system. We develop an Observability-Constrained VINS (OC-VINS), which explicitly enforces the unobservable directions of the system, hence preventing spurious informa-tion gain and reducing inconsistency. Our analysis, along with the proposed method for reducing inconsistency, are extensively validated with simulation trials and real-world experimentation.},
author = {Hesch, Joel A and Kottas, Dimitrios G and Bowman, Sean L and Roumeliotis, Stergios I},
journal = {University of Minnesota, Dept. of Comp. Sci. & Eng., MARS Lab, Tech. Rep},
number = {612},
title = {{Observability-constrained vision-aided inertial navigation}},
volume = {1},
year = {2012}
}
@article{Keetha2023,
abstract = {Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.},
archivePrefix = {arXiv},
arxivId = {2312.02126},
author = {Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan, Deva and Luiten, Jonathon},
eprint = {2312.02126},
title = {{SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM}},
url = {http://arxiv.org/abs/2312.02126},
year = {2023}
}
@article{Ouabi2022a,
abstract = {The inspection of sizeable plate-based metal structures such as storage tanks or marine vessel hulls is a significant stake in the industry, which necessitates reliable and time-efficient solutions. Although Lamb waves have been identified as a promising solution for long-range non-destructive testing, and despite the substantial progress made in autonomous navigation and environment sensing, a Lamb-wave-based robotic system for extensive structure monitoring is still lacking. Following previous work on ultrasonic Simultaneous Localization and Mapping (SLAM), we introduce a method to achieve plate geometry inference without prior knowledge of the material propagation properties, which may be lacking during a practical inspection task in challenging and outdoor environments. Our approach combines focalization to adjust the propagation model parameters and beamforming to infer the plate boundaries location by relying directly on acoustic measurements acquired along the mobile unit trajectory. For each candidate model, the focusing ability of the corresponding beamformer is assessed over high-pass filtered beamforming maps to further improve the robustness of the plate geometry estimates. We then recover the optimal space-domain beamformer through a simulated annealing optimization process. We evaluate our method on three sets of experimental data acquired in different conditions and show that accurate plate geometry inference can be achieved without any prior propagation model. Finally, the results show that the optimal beamformer outperforms the beamformer resulting from the predetermined propagation model in non-nominal acquisition conditions.},
archivePrefix = {arXiv},
arxivId = {arXiv:2107.05842v1},
author = {Ouabi, Othmane-Latif and Pomarede, Pascal and Declercq, Nico and Zeghidour, Neil and Geist, Matthieu and Pradalier, C{\'{e}}dric and Declercq, Nico F and {Edric Pradalier}, C ´},
doi = {10.1177/ToBeAssigned},
eprint = {arXiv:2107.05842v1},
journal = {Ultrasonics},
keywords = {Acoustic mapping,Helmholtz equation,Lamb waves,Model learning,Optimal beamforming},
number = {...},
pages = {106705},
title = {{Learning the Propagation Properties of Plate-like Structures for Lamb Wave-based Mapping Learn-ing the Propagation Properties of Plate-like Structures for Lamb Wave-based Mapping Learning the Propagation Properties of Plate-like Structures for Lamb Wave-b}},
volume = {...},
year = {2022}
}
@article{Chen,
author = {Chen, Chuchu and Geneva, Patrick and Peng, Yuxiang and Lee, Woosik and Huang, Guoquan},
title = {{Monocular Visual-Inertial Odometry with Planar Regularities}}
}
@article{Kirillov2023c,
abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
archivePrefix = {arXiv},
arxivId = {2304.02643},
author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'{a}}r, Piotr and Girshick, Ross},
eprint = {2304.02643},
title = {{Segment Anything}},
url = {http://arxiv.org/abs/2304.02643},
year = {2023}
}
@article{Zhang,
author = {Zhang, Zichao and Forster, Christian and Scaramuzza, Davide},
title = {{Active Exposure}}
}
@article{Nava2018,
abstract = {State-of-the-art LIDAR odometry techniques are exceptionally precise. However, while they solve the localization problem, they perform mapping on-the-run, not being able to close loops, neither re- ...},
author = {Nava, Yoshua},
journal = {Degree Project Computer Science and Engineering},
keywords = {Computer and Information Sciences,Data,och informationsvetenskap},
number = {June 2018},
title = {{Visual-LiDAR SLAM with loop closure}},
url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-265532},
year = {2018}
}
@article{Han2022,
abstract = {Over the last decade, multi-tasking learning approaches have achieved promising results in solving panoptic driving perception problems, providing both high-precision and high-efficiency performance. It has become a popular paradigm when designing networks for real-time practical autonomous driving system, where computation resources are limited. This paper proposed an effective and efficient multi-task learning network to simultaneously perform the task of traffic object detection, drivable road area segmentation and lane detection. Our model achieved the new state-of-the-art (SOTA) performance in terms of accuracy and speed on the challenging BDD100K dataset. Especially, the inference time is reduced by half compared to the previous SOTA model. Code will be released in the near future.},
archivePrefix = {arXiv},
arxivId = {2208.11434},
author = {Han, Cheng and Zhao, Qichao and Zhang, Shuyi and Chen, Yinzi and Zhang, Zhenlin and Yuan, Jinwei},
eprint = {2208.11434},
title = {{YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception}},
url = {http://arxiv.org/abs/2208.11434},
year = {2022}
}
@article{Matsuki2023,
abstract = {We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera. Our Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation, but also reconstruction of tiny and even transparent objects.},
archivePrefix = {arXiv},
arxivId = {2312.06741},
author = {Matsuki, Hidenobu and Murai, Riku and Kelly, Paul H. J. and Davison, Andrew J.},
eprint = {2312.06741},
title = {{Gaussian Splatting SLAM}},
url = {http://arxiv.org/abs/2312.06741},
year = {2023}
}
@article{Huang2008d,
abstract = {In this work, we study the inconsistency problem of EKF-based SLAM from the perspective of observability. We analytically prove that when the Jacobians of the system and measurement models are evaluated at the latest state estimates during every time step, the linearized error-state system employed in the EKF has observable subspace of dimension higher than that of the actual, nonlinear, SLAM system. As a result, the covariance estimates of the EKF undergo reduction in directions of the state space where no information is available, which is a primary cause of the inconsistency. Based on these theoretical results, we propose a general framework for improving the consistency of EKF-based SLAM. In particular, we propose selecting the EKF linearization points in a way that ensures that the resulting linearized system model has an observable subspace of appropriate dimension. This can be achieved by calculating the filter Jacobians using the first-ever available estimates for each state variable. The resulting "First-Estimates Jacobian" (FEJ) EKF has been tested both in simulation and in real-world experiments, and is shown to significantly outperform the standard EKF both in terms of accuracy and consistency.},
author = {Huang, Guoquan P and Mourikis, Anastasios I and Roumeliotis, Stergios I},
isbn = {9781424416479},
journal = {IEEE Int. Conf. Robotics and Automation (ICRA)},
keywords = {Localization,SLAM,Sensor Fusion},
number = {612},
pages = {473--479},
title = {{Generalized Analysis and Improvement of the Consistency of EKF-based SLAM}},
year = {2008}
}
@article{Furgale2011b,
abstract = {Mars represents one of the most important targets for space exploration in the next 10 to 30 years, particularly because of evidence of liquid water in the planet's past. Current environmental conditions dictate that any existing water reserves will be in the form of ice; finding and sampling these ice deposits would further the study of the planet's climate history, further the search for evidence of life, and facilitate in-situ resource utilization during future manned exploration missions. This thesis presents a suite of algorithms to help enable a robotic ice-prospecting mission to Mars. Starting from visual odometry--the estimation of a rover's motion using a stereo camera as the primary sensor--we develop the following extensions: (i) a coupled surface/subsurface modelling system that provides novel data products to scientists working remotely, (ii) an autonomous retrotraverse system that allows a rover to return to previously visited places along a route for sampling, or to return a sample to an ascent vehicle, and (iii) the extension of the appearance-based visual odometry pipeline to an actively illuminated light detection and ranging sensor that provides data similar to a stereo camera but is not reliant on consistent ambient lighting, thereby enabling appearance-based vision techniques to be used in environments that are not conducive to passive cameras, such as underground mines or permanently shadowed craters on the moon. All algorithms are evaluated on real data collected using our field robot at the University of Toronto Institute for Aerospace Studies, or at a planetary analogue site on Devon Island, in the Canadian High Arctic.},
author = {Furgale, Paul Timothy},
isbn = {9780494781852},
issn = {0494781858},
journal = {ProQuest Dissertations and Theses},
keywords = {0538:Aerospace engineering,0771:Robotics,Aerospace engineering,Applied sciences,Planetary surfaces,Robotic missions,Robotics,Space exploration,Visual odometry},
pages = {157},
title = {{Extensions to the Visual Odometry Pipeline for the Exploration of Planetary Surfaces}},
url = {http://ezproxy.net.ucf.edu/login?url=http://search.proquest.com/docview/924428359?accountid=10003%5Cnhttp://sfx.fcla.edu/ucf?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations+&+theses&sid=ProQ:ProQuest+Dissertations+&+T},
volume = {NR78185},
year = {2011}
}
@article{Hartley,
author = {Hartley, Richard and Zisserman, Andrew},
title = {{Geom{\`{e}}try in Computer Vision 中 多 中}}
}
@article{Liu2022a,
abstract = {We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer by layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer by layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at https://github.com/IDEA-opensource/DAB-DETR.},
archivePrefix = {arXiv},
arxivId = {2201.12329},
author = {Liu, Shilong and Li, Feng and Zhang, Hao and Yang, Xiao and Qi, Xianbiao and Su, Hang and Zhu, Jun and Zhang, Lei},
eprint = {2201.12329},
journal = {ICLR 2022 - 10th International Conference on Learning Representations},
pages = {1--19},
title = {{Dab-Detr: Dynamic Anchor Boxes Are Better Queries for Detr}},
year = {2022}
}
@article{Mourikis2011,
abstract = {This report focuses on motion estimation using inertial measurements and observations of naturally occurring point features. To date, this task has primarily been addressed using filtering methods, which track the system state starting from known initial conditions. However, when no prior knowledge of the initial system state is available, (e.g., at the onset of the system's operation), the existing approaches are not applicable. To address this problem, in this work we present algorithms for computing all the observable quantities (platform attitude and velocity, feature positions, IMU biases, and IMU-camera calibration) in closed form directly from the sensor measurements, without any prior knowledge. As a key contribution of this work, we identify and analyze the properties of minimal problems that have a finite number of solutions, as well as singular trajectories, in which solutions cannot be computed. Additionally, to address the presence of noise in the measurements, we present a quadratically constrained least-squares solution and an iterative maximum-likelihood estimator.},
author = {Mourikis, Anastasios I},
journal = {IEEE International Conference on Intelligent Robots and System},
title = {{Closed-form Solutions for Vision-aided Inertial Navigation}},
year = {2011}
}
@article{Fan2023a,
abstract = {Scaling to arbitrarily large bundle adjustment problems requires data and compute to be distributed across multiple devices. Centralized methods in prior works are only able to solve small or medium size problems due to overhead in computation and communication. In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems. We achieve this by reformulating the reprojection error and deriving a novel surrogate function that decouples optimization variables from different devices. This function makes it possible to use majorization minimization techniques and reduces bundle adjustment to independent optimization subproblems that can be solved in parallel. We further apply Nesterov's acceleration and adaptive restart to improve convergence while maintaining its theoretical guarantees. Despite limited peer-to-peer communication, our method has provable convergence to first-order critical points under mild conditions. On extensive benchmarks with public datasets, our method converges much faster than decentralized baselines with similar memory usage and communication load. Compared to centralized baselines using a single device, our method, while being decentralized, yields more accurate solutions with significant speedups of up to 940.7x over Ceres and 175.2x over DeepLM. Code: https://github.com/facebookresearch/DBA.},
archivePrefix = {arXiv},
arxivId = {2305.07026},
author = {Fan, Taosha and Ortiz, Joseph and Hsiao, Ming and Monge, Maurizio and Dong, Jing and Murphey, Todd and Mukadam, Mustafa},
eprint = {2305.07026},
title = {{Decentralization and Acceleration Enables Large-Scale Bundle Adjustment}},
url = {http://arxiv.org/abs/2305.07026},
year = {2023}
}
@article{Xie2021,
abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5× smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.},
archivePrefix = {arXiv},
arxivId = {2105.15203},
author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
eprint = {2105.15203},
isbn = {9781713845393},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {12077--12090},
title = {{SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers}},
volume = {15},
year = {2021}
}
@article{Blanco-Claraco2021a,
abstract = {An arbitrary rigid transformation in $\backslashmathbf{\{}SE{\}}(3)$ can be separated into two parts, namely, a translation and a rigid rotation. This technical report reviews, under a unifying viewpoint, three common alternatives to representing the rotation part: sets of three (yaw-pitch-roll) Euler angles, orthogonal rotation matrices from $\backslashmathbf{\{}SO{\}}(3)$ and quaternions. It will be described: (i) the equivalence between these representations and the formulas for transforming one to each other (in all cases considering the translational and rotational parts as a whole), (ii) how to compose poses with poses and poses with points in each representation and (iii) how the uncertainty of the poses (when modeled as Gaussian distributions) is affected by these transformations and compositions. Some brief notes are also given about the Jacobians required to implement least-squares optimization on manifolds, an very promising approach in recent engineering literature. The text reflects which MRPT C++ library functions implement each of the described algorithms. All formulas and their implementation have been thoroughly validated by means of unit testing and numerical estimation of the Jacobians},
archivePrefix = {arXiv},
arxivId = {2103.15980},
author = {Blanco-Claraco, Jos{\'{e}} Luis},
eprint = {2103.15980},
number = {3},
title = {{A tutorial on $\backslashmathbf{\{}SE{\}}(3)$ transformation parameterizations and on-manifold optimization}},
url = {http://arxiv.org/abs/2103.15980},
year = {2021}
}
@article{Estimation2018,
author = {Estimation, Covariance},
title = {{Marginazation Factor 舒尔补与边缘概率}},
year = {2018}
}
@article{Xiaochen2021,
author = {Xiaochen, Qiu},
title = {邱博-预积分总结与公式推导},
year = {2021}
}
@article{Nerurkar,
author = {Nerurkar, Esha D and Wu, Kejian J and Roumeliotis, Stergios I},
title = {{C-KLAM : Constrained Keyframe-Based Localization and Mapping}}
}
@article{Wen2021,
abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack},
archivePrefix = {arXiv},
arxivId = {2108.00516},
author = {Wen, Bowen and Bekris, Kostas},
eprint = {2108.00516},
title = {{BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models}},
url = {http://arxiv.org/abs/2108.00516},
year = {2021}
}
@article{Xiong2023,
abstract = {Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., $\sim$4 AP on COCO/LVIS) over other fast SAM models.},
archivePrefix = {arXiv},
arxivId = {2312.00863},
author = {Xiong, Yunyang and Varadarajan, Bala and Wu, Lemeng and Xiang, Xiaoyu and Xiao, Fanyi and Zhu, Chenchen and Dai, Xiaoliang and Wang, Dilin and Sun, Fei and Iandola, Forrest and Krishnamoorthi, Raghuraman and Chandra, Vikas},
eprint = {2312.00863},
title = {{EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything}},
url = {http://arxiv.org/abs/2312.00863},
year = {2023}
}
@book{Gehring,
abstract = {This new book offers a fresh approach to matrix and linear algebra by providing a balanced blend of applications, theory, and computation, while highlighting their interdependence. Intended for a one-semester course, Applied Linear Algebra and Matrix Analysis places special emphasis on linear algebra as an experimental science, with numerous examples, computer exercises, and projects. While the flavor is heavily computational and experimental, the text is independent of specific hardware or software platforms. Throughout the book, significant motivating examples are woven into the text, and each section ends with a set of exercises. The student will develop a solid foundation in the following topics: Gaussian elimination and other operations with matrices; basic properties of matrix and determinant algebra; standard Euclidean spaces, both real and complex; geometrical aspects of vectors, such as norm, dot product, and angle; eigenvalues, eigenvectors, and discrete dynamical systems; general norm and inner-product concepts for abstract vector spaces. For many students, the tools of matrix and linear algebra will be as fundamental in their professional work as the tools of calculus; thus it is important to ensure that students appreciate the utility and beauty of these subjects as well as the mechanics. By including applied mathematics and mathematical modeling, this new textbook will teach students how concepts of matrix and linear algebra make concrete problems workable. Thomas S. Shores is Professor of Mathematics at the University of Nebraska, Lincoln, where he has received awards for his teaching. His research touches on group theory, commutative algebra, mathematical modeling, numerical analysis, and inverse theory},
author = {Gehring, F W and Halmos, P R and Deprima, C and Herstein, I and Kiefer, J and Leveque, W},
isbn = {3540901639},
title = {{Undergraduate Texts in Mathematics}}
}
@article{Xiaochen2019,
author = {Xiaochen, Qiu},
title = {{采用Hamilton四元数的低成本IMU误差方程详细推导}},
year = {2019}
}
@article{Wen2021,
abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack},
archivePrefix = {arXiv},
arxivId = {2108.00516},
author = {Wen, Bowen and Bekris, Kostas},
eprint = {2108.00516},
title = {{BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models}},
url = {http://arxiv.org/abs/2108.00516},
year = {2021}
}
@article{Oquab2023,
abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
archivePrefix = {arXiv},
arxivId = {2304.07193},
author = {Oquab, Maxime and Darcet, Timoth{\'{e}}e and Moutakanni, Th{\'{e}}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'{e}} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
eprint = {2304.07193},
pages = {1--32},
title = {{DINOv2: Learning Robust Visual Features without Supervision}},
url = {http://arxiv.org/abs/2304.07193},
year = {2023}
}
@article{Miao2023,
abstract = {3D Semantic Scene Completion (SSC) can provide dense geometric and semantic scene representations, which can be applied in the field of autonomous driving and robotic systems. It is challenging to estimate the complete geometry and semantics of a scene solely from visual images, and accurate depth information is crucial for restoring 3D geometry. In this paper, we propose the first stereo SSC method named OccDepth, which fully exploits implicit depth information from stereo images (or RGBD images) to help the recovery of 3D geometric structures. The Stereo Soft Feature Assignment (Stereo-SFA) module is proposed to better fuse 3D depth-aware features by implicitly learning the correlation between stereo images. In particular, when the input are RGBD image, a virtual stereo images can be generated through original RGB image and depth map. Besides, the Occupancy Aware Depth (OAD) module is used to obtain geometry-aware 3D features by knowledge distillation using pre-trained depth models. In addition, a reformed TartanAir benchmark, named SemanticTartanAir, is provided in this paper for further testing our OccDepth method on SSC task. Compared with the state-of-the-art RGB-inferred SSC method, extensive experiments on SemanticKITTI show that our OccDepth method achieves superior performance with improving +4.82% mIoU, of which +2.49% mIoU comes from stereo images and +2.33% mIoU comes from our proposed depth-aware method. Our code and trained models are available at https://github.com/megvii-research/OccDepth.},
archivePrefix = {arXiv},
arxivId = {2302.13540},
author = {Miao, Ruihang and Liu, Weizhou and Chen, Mingrui and Gong, Zheng and Xu, Weixin and Hu, Chen and Zhou, Shuchang},
eprint = {2302.13540},
title = {{OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion}},
url = {http://arxiv.org/abs/2302.13540},
year = {2023}
}
